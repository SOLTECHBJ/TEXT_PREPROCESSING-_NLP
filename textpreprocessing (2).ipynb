{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ccd587b6",
   "metadata": {},
   "source": [
    "# READ  SQUAD DATASET\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e4ab90b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b8895d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.read_json('train-v2.0.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a4f67c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2c21361",
   "metadata": {},
   "source": [
    "# EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130262b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install pandas matplotlib\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7df9a2ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1386668d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e412bac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dedc239f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6794cb4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "null_values=df.isnull().sum\n",
    "null_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "125cdd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "##TEXT PREPROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ed3a16",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_length = df.applymap(lambda x: len(str(x)))\n",
    "\n",
    "\n",
    "print(data_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdeae4f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_column=df.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38c154d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbf24ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming df is your DataFrame\n",
    "for index, row in df.iterrows():\n",
    "    print(row)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e272899",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_df = df.drop(columns=['version'])\n",
    "new_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a03ae5e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "##lowercase\n",
    "import json\n",
    "with open('train-v2.0.json', 'r') as f:\n",
    "    squad_data = json.load(f)\n",
    "\n",
    "# Iterate through each item in the dataset\n",
    "for item in squad_data['data']:\n",
    "    # Iterate through paragraphs\n",
    "    for paragraph in item['paragraphs']:\n",
    "        # Convert context to lowercase\n",
    "        paragraph['context'] = paragraph['context'].lower()\n",
    "        \n",
    "        # Iterate through questions and answers\n",
    "        for qa in paragraph['qas']:\n",
    "            # Convert question to lowercase\n",
    "            qa['question'] = qa['question'].lower()\n",
    "            \n",
    "            # Convert answers to lowercase\n",
    "            for answer in qa['answers']:\n",
    "                answer['text'] = answer['text'].lower()\n",
    "\n",
    "# Save the modified dataset\n",
    "with open('train-v2.0.json', 'w') as f:\n",
    "    json.dump(squad_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3be9038",
   "metadata": {},
   "outputs": [],
   "source": [
    "squad_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b045bb9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafeba4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    pattern = re.compile(r'<[^>]*>')\n",
    "    return pattern.sub('', text)\n",
    "\n",
    "# Test the function\n",
    "text = \"<body><p> my name is bharti jha </p><a href='https:'</body>\"\n",
    "clean_text = remove_html_tags(text)\n",
    "print(clean_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9a0faf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_html_tags(text):\n",
    "    \n",
    "    if isinstance(text, str):\n",
    "        pattern = re.compile(r'<[^>]*>')\n",
    "        return pattern.sub('', text)\n",
    "    else:\n",
    "       \n",
    "        return text\n",
    "\n",
    "for item in squad_data['data']:\n",
    "    for paragraph in item['paragraphs']:\n",
    "        paragraph['context'] = remove_html_tags(paragraph['context'])\n",
    "        for qa in paragraph['qas']:\n",
    "            qa['question'] = remove_html_tags(qa['question'])\n",
    "            for answer in qa['answers']:\n",
    "                answer['text'] = remove_html_tags(answer['text'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e26e64",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\" *******STRUCTURE OF SQUAD DATASET*******\\n\")\n",
    "def print_structure(data, indent=0):\n",
    "    for key, value in data.items():\n",
    "        print('  ' * indent + str(key))\n",
    "        if isinstance(value, dict):\n",
    "            print_structure(value, indent + 1)\n",
    "        elif isinstance(value, list) and len(value) > 0 and isinstance(value[0], dict):\n",
    "            print_structure(value[0], indent + 1)\n",
    "\n",
    "# Assuming squad_data is your SQuAD dataset dictionary\n",
    "print_structure(squad_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d36aa25",
   "metadata": {},
   "outputs": [],
   "source": [
    "answers_of_6th_item = squad_data['data'][5]['title']\n",
    "answers_of_6th_item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4794de78",
   "metadata": {},
   "outputs": [],
   "source": [
    "item = squad_data['data'][5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a641c732",
   "metadata": {},
   "outputs": [],
   "source": [
    "item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2898372",
   "metadata": {},
   "outputs": [],
   "source": [
    "#removing urls\n",
    "import re\n",
    "\n",
    "def remove_urls(text):\n",
    "    \n",
    "    url_pattern = re.compile(r'https?://\\S+|www\\.\\S+')\n",
    "    \n",
    "\n",
    "    text_without_urls = url_pattern.sub('', text)\n",
    "    \n",
    "    return text_without_urls\n",
    "\n",
    "for item in squad_data['data']:\n",
    "    for paragraph in item['paragraphs']:\n",
    "        paragraph['context'] = remove_urls(paragraph['context'])\n",
    "        for qa in paragraph['qas']:\n",
    "            qa['question'] = remove_urls(qa['question'])\n",
    "            for answer in qa['answers']:\n",
    "                answer['text'] = remove_urls(answer['text'])\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "291803c5",
   "metadata": {},
   "source": [
    "# PUNCTUATIONS (SPECIAL KEWORDS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5fd3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "###punctuation{?!}\n",
    "import string\n",
    "import time\n",
    "\n",
    "punctuation = string.punctuation\n",
    "\n",
    "\n",
    "current_time = time.time()\n",
    "\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ca01503",
   "metadata": {},
   "outputs": [],
   "source": [
    "exclude=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb943695",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"***remove punctuation***\")\n",
    "def remove_punc(text):\n",
    "    for char in exclude:\n",
    "        text=text.replace(char, '')\n",
    "    return text   \n",
    "text=\"???!b(harti) jha\"\n",
    "start=time.time()\n",
    "print(remove_punc(text))\n",
    "time1=time.time()-start\n",
    "time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f047fda8",
   "metadata": {},
   "outputs": [],
   "source": [
    "start=time.time()\n",
    "print(remove_punc(text))\n",
    "time1=time.time()-start\n",
    "time1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3eda9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a2dd03",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"***remove punctuations from dataset***\")\n",
    "import string\n",
    "exclude=string.punctuation\n",
    "def remove_punct(text):\n",
    "    return text.translate(str.maketrans('','',exclude))\n",
    "\n",
    "# Iterate through the SQuAD dataset and apply remove_punctuation() function to relevant text fields\n",
    "for item in squad_data['data']:\n",
    "    for paragraph in item['paragraphs']:\n",
    "        # Remove punctuation from the context\n",
    "        paragraph['context'] = remove_punct(paragraph['context'])\n",
    "        for qa in paragraph['qas']:\n",
    "            # Remove punctuation from the questions\n",
    "            qa['question'] = remove_punct(qa['question'])\n",
    "            for answer in qa['answers']:\n",
    "                # Remove punctuation from the answers\n",
    "                answer['text'] = remove_punct(answer['text'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e91a4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "answer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b253922",
   "metadata": {},
   "source": [
    "# CHAT_WORD_TREATMENT(ASAP,OMG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88cfdcd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_str = [\n",
    "    \"BRB=Be Right Back\",\n",
    "    \"LOL=Laughing Out Loud\",\n",
    "    \"TTYL=Talk To You Later\",\n",
    "    \"SMH=Shaking My Head\",\n",
    "    \"AFK=Away From Keyboard\",\n",
    "    \"ROFL=Rolling On the Floor Laughing\",\n",
    "    \"IMO=In My Opinion\",\n",
    "    \"IMHO=In My Humble Opinion\",\n",
    "    \"AF=As ***l\",\n",
    "    \"AFAIK=As Far As I Know\",\n",
    "    \"ASAP=As Soon As Possible\",\n",
    "    \"B4=Before\",\n",
    "    \"CUL8R=See You Later\",\n",
    "    \"DIY=Do It Yourself\",\n",
    "    \"F2F=Face To Face\",\n",
    "    \"FAQ=Frequently Asked Questions\",\n",
    "    \"FB=Facebook\",\n",
    "    \"FYI=For Your Information\",\n",
    "    \"G2G=Got To Go\",\n",
    "    \"GMTA=Great Minds Think Alike\",\n",
    "    \"HAND=Have A Nice Day\",\n",
    "    \"HF=Have Fun\",\n",
    "    \"IDC=I Don't Care\",\n",
    "    \"IDGAF=I Don't Give A ***k\",\n",
    "    \"IDK=I Don't Know\",\n",
    "    \"IDTS=I Don't Think So\",\n",
    "    \"IIRC=If I Remember Correctly\",\n",
    "    \"IMO=In My Opinion\",\n",
    "    \"IMHO=In My Humble Opinion\",\n",
    "    \"IRL=In Real Life\",\n",
    "    \"JFYI=Just For Your Information\",\n",
    "    \"K=Okay\",\n",
    "    \"LMAO=Laughing My *** Off\",\n",
    "    \"LMK=Let Me Know\",\n",
    "    \"NVM=Never Mind\",\n",
    "    \"OMG=Oh My God\",\n",
    "    \"OMW=On My Way\",\n",
    "    \"PITA=Pain In The ***s\",\n",
    "    \"PLS=Please\",\n",
    "    \"ROTFL=Rolling On The Floor Laughing\",\n",
    "    \"RTFM=Read The *** Manual\",\n",
    "    \"SOS=Save Our Souls\",\n",
    "    \"STFU=Shut The *** Up\",\n",
    "    \"TIA=Thanks In Advance\",\n",
    "    \"TMI=Too Much Information\",\n",
    "    \"TY=Thank You\",\n",
    "    \"U=You\",\n",
    "    \"UR=Your\",\n",
    "    \"WB=Welcome Back\",\n",
    "    \"WTH=What The ***k\",\n",
    "    \"W8=Wait\",\n",
    "    \"YW=You're Welcome\",\n",
    "    \"1CE=Once\",\n",
    "    \"1DR=Wonder\",\n",
    "    \"1LY=Lovely\",\n",
    "    \"1NAM=One Name\",\n",
    "    \"1NC=Once\",\n",
    "    \"1PPL=One People\"\n",
    "]\n",
    "\n",
    "\n",
    "chat_words_dict = {}\n",
    "\n",
    "# Iterate over each string in the list\n",
    "for entry in chat_words_str:\n",
    "    # Split the string by '=' to separate chat word and expansion\n",
    "    chat_word, expansion = entry.split(\"=\")\n",
    "    # Add chat word and expansion to the dictionary\n",
    "    chat_words_dict[chat_word] = expansion\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b7e05fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153c6db4",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_words_count = len(chat_words_dict)\n",
    "chat_words_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4347ca9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat_conversion(text):\n",
    "    new_text=[]\n",
    "    for w in text.split():\n",
    "        if w.upper() in chat_words_dict:\n",
    "            new_text.append(chat_words_dict[w.upper()])\n",
    "        else:\n",
    "            new_text.append(w)\n",
    "    return \" \".join(new_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38608d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_conversion('IMHO HE IS THE BEST')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffcedf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_conversion('faq')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62104351",
   "metadata": {},
   "source": [
    "# SPELLING CORRECTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d51b95",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c1f0c3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"****Corrected text****\")\n",
    "from textblob import TextBlob\n",
    "\n",
    "misspellings = 'youu aree intelligent'\n",
    "corrected_text = TextBlob(misspellings).correct().string\n",
    "print(corrected_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53449ea",
   "metadata": {},
   "source": [
    "# REMOVE STOPWORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7d5b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a454ec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d03770",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"****Total stowords****\")\n",
    "stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a789ce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"@@@@ REMOVE STOPWORDS FROM TEXT @@@@\")\n",
    "def remove_stopwords(text):\n",
    "    new_text=[]\n",
    "    for word in text.split():\n",
    "        if word in stopwords.words('english'):\n",
    "            new_text.append('')\n",
    "        else:\n",
    "            new_text.append(word)\n",
    "    x=new_text[:]\n",
    "    new_text.clear()\n",
    "    return \" \".join(x)\n",
    "print(\"****with stopwords text****\\n\")\n",
    "print(\"my name is a bharti jha i am from patna bihar india\\n\")\n",
    "print(\"****without stopwords text****\\n\")\n",
    "hello=remove_stopwords(\"my name is a bharti jha. i am from patna bihar india\")\n",
    "hello"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d29ef0d2",
   "metadata": {},
   "source": [
    "# HANDLING EMOZIS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef38e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def remove_emojis(text):\n",
    "    # Define a regular expression pattern to match emojis\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  \n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  \n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  \n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  \n",
    "                               u\"\\U00002500-\\U00002BEF\"  \n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U00002702-\\U000027B0\"\n",
    "                               u\"\\U000024C2-\\U0001F251\"\n",
    "                               u\"\\U0001f926-\\U0001f937\"\n",
    "                               u\"\\U00010000-\\U0010ffff\"\n",
    "                               u\"\\u2640-\\u2642\"\n",
    "                               u\"\\u2600-\\u2B55\"\n",
    "                               u\"\\u200d\"\n",
    "                               u\"\\u23cf\"\n",
    "                               u\"\\u23e9\"\n",
    "                               u\"\\u231a\"\n",
    "                               u\"\\ufe0f\"  \n",
    "                               u\"\\u3030\"\n",
    "                               u\"\\U0001F004-\\U0001F0CF\"  \n",
    "                               u\"\\U0001F170-\\U0001F251\"\n",
    "                               u\"\\U0001F300-\\U0001F320\"  \n",
    "                               u\"\\U0001F330-\\U0001F335\"  \n",
    "                               u\"\\U0001F337-\\U0001F37C\"  \n",
    "                                u\"\\U0001F380-\\U0001F393\"  \n",
    "                            u\"\\U0001F3A0-\\U0001F3CA\"\n",
    "                            u\"\\U0001F400-\\U0001F43E\"  \n",
    "                            u\"\\U0001F440-\\U0001F4FD\"\n",
    "                            u\"\\U0001F600-\\U0001F64F\"  \n",
    "                            u\"\\U0001F680-\\U0001F6C5\"  \n",
    "                            u\"\\U0001F700-\\U0001F773\"  \n",
    "                            u\"\\U0001F780-\\U0001F7D8\"  \n",
    "                            u\"\\U0001F800-\\U0001F80B\"  \n",
    "                            u\"\\U0001F900-\\U0001F9FF\"  \n",
    "                            u\"\\U0001FA00-\\U0001FA6D\"  \n",
    "                            u\"\\U0001FA70-\\U0001FA74\"  \n",
    "                            u\"\\U0001FA78-\\U0001FA7A\"  \n",
    "                            u\"\\U0001FA80-\\U0001FA86\"  \n",
    "                            u\"\\U0001FA90-\\U0001FAA8\"  \n",
    "                            u\"\\U0001FAC0-\\U0001FAC2\"  \n",
    "                            u\"\\U0001FAD0-\\U0001FAD6\"  \n",
    "                            u\"\\U0001F4FF\"              \n",
    "\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "   \n",
    "    return emoji_pattern.sub(r'', text)\n",
    "\n",
    "text_with_emojis = \"Hello! ðŸ˜Š How are you? ðŸŒŸ\"\n",
    "text_without_emojis = remove_emojis(text_with_emojis)\n",
    "print(\"Text with emojis:\", text_with_emojis)\n",
    "print(\"Text without emojis:\", text_without_emojis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39b48645",
   "metadata": {},
   "outputs": [],
   "source": [
    "emozi_pattern = (\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "    u\"\\U00002500-\\U00002BEF\"  # Chinese Characters\n",
    "    u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    u\"\\U000024C2-\\U0001F251\"  # Enclosed Characters\n",
    "    u\"\\U0001f926-\\U0001f937\"  # Additional Facial Expressions\n",
    "    u\"\\U00010000-\\U0010ffff\"  # Supplementary Characters\n",
    "    u\"\\u2640-\\u2642\"          # Gender Symbols\n",
    "    u\"\\u2600-\\u2B55\"          # Miscellaneous Symbols\n",
    "    u\"\\u200d\"                 # Zero Width Joiner\n",
    "    u\"\\u23cf\"                 # Eject Symbol\n",
    "    u\"\\u23e9\"                 # Fast Forward Symbol\n",
    "    u\"\\u231a\"                 # Watch Symbol\n",
    "    u\"\\U0001F004-\\U0001F0CF\"  # Misc. Symbols and Pictographs\n",
    "    u\"\\U0001F170-\\U0001F251\"  # Enclosed Characters\n",
    "    u\"\\U0001F300-\\U0001F320\"  # Various Weather Symbols\n",
    "    u\"\\U0001F330-\\U0001F335\"  # Vegetables & Fruits\n",
    "    u\"\\U0001F337-\\U0001F37C\"  # Food & Drink\n",
    "    u\"\\U0001F380-\\U0001F393\"  # Celebration & Events\n",
    "    u\"\\U0001F3A0-\\U0001F3CA\"  # Entertainment\n",
    "    u\"\\U0001F400-\\U0001F43E\"  # Animals & Nature\n",
    "    u\"\\U0001F440-\\U0001F4FD\"  # Facial expressions & Body parts\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    "    u\"\\U0001F680-\\U0001F6C5\"  # Transport & Map Symbols\n",
    "    u\"\\U0001F700-\\U0001F773\"  # Alchemical Symbols\n",
    "    u\"\\U0001F780-\\U0001F7D8\"  # Geometric Shapes Extended\n",
    "    u\"\\U0001F800-\\U0001F80B\"  # Supplemental Arrows-C\n",
    "    u\"\\U0001F900-\\U0001F9FF\"  # Supplemental Symbols and Pictographs\n",
    "    u\"\\U0001FA00-\\U0001FA6D\"  # Chess Symbols\n",
    "    u\"\\U0001FA70-\\U0001FA74\"  # Symbols for Legacy Computing\n",
    "    u\"\\U0001FA78-\\U0001FA7A\"  # Symbols for Legacy Computing\n",
    "    u\"\\U0001FA80-\\U0001FA86\"  # Symbols for Legacy Computing\n",
    "    u\"\\U0001FA90-\\U0001FAA8\"  # Symbols for Legacy Computing\n",
    "    u\"\\U0001FAC0-\\U0001FAC2\"  # Symbols for Legacy Computing\n",
    "    u\"\\U0001FAD0-\\U0001FAD6\"  # Symbols for Legacy Computing\n",
    "    u\"\\U0001F4FF\"              # Praying Hands (Not official emoji)\n",
    "    ##DUPLICATION\n",
    "    u\"\\U0001F300-\\U0001F5FF\"  # Symbols & Pictographs\n",
    "    u\"\\U0001F680-\\U0001F6FF\"  # Transport & Map Symbols\n",
    "    u\"\\U0001F1E0-\\U0001F1FF\"  # Flags (iOS)\n",
    "    u\"\\U00002500-\\U00002BEF\"  # Chinese Characters\n",
    "    u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    u\"\\U00002702-\\U000027B0\"  # Dingbats\n",
    "    u\"\\U000024C2-\\U0001F251\"  # Enclosed Characters\n",
    "    u\"\\U0001F600-\\U0001F64F\"  # Emoticons\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76dc5971",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('DUPLICATION SHOW EMOZY')\n",
    "def find_repeated_patterns(emozi_pattern):\n",
    "    # Compile a regular expression pattern to find repeated occurrences of emoji patterns\n",
    "    pattern = re.compile(r'([^\\s]+).*?\\1')\n",
    "\n",
    "    # Search for repeated patterns in the emoji pattern string\n",
    "    repeated_patterns = pattern.findall(emozi_pattern)\n",
    "\n",
    "    return repeated_patterns\n",
    "# Find repeated emoji patterns\n",
    "repeated_patterns = find_repeated_patterns(emozi_pattern)\n",
    "\n",
    "# Print the repeated emoji patterns\n",
    "if repeated_patterns:\n",
    "    print(\"Repeated emoji patterns found:\")\n",
    "    for pattern in repeated_patterns:\n",
    "        print(pattern)\n",
    "else:\n",
    "    print(\"No repeated emoji patterns found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965346f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('@@@@@@ DUPLICATION CLEANED EMOZY @@@@@@\\n')\n",
    "def remove_repeated_patterns(emozi_pattern):\n",
    "    pattern = re.compile(r'([^\\s]+).*?\\1')\n",
    "\n",
    "    # Remove repeated patterns from the emoji pattern string\n",
    "    cleaned_pattern = pattern.sub('', emozi_pattern)\n",
    "\n",
    "    return cleaned_pattern\n",
    "# Remove repeated emoji patterns FUNCTION CALL\n",
    "cleaned_pattern = remove_repeated_patterns(emozi_pattern)\n",
    "\n",
    "print(\"Emoji pattern after removing repeated patterns:\")\n",
    "print(cleaned_pattern)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a21e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_emozi_names():\n",
    "    # Define a dictionary mapping emoji patterns to their names\n",
    "    emozi_names = {\n",
    "        u\"\\U0001F600-\\U0001F64F\": \"Emoticons\",\n",
    "        u\"\\U0001F300-\\U0001F5FF\": \"Symbols & Pictographs\",\n",
    "        u\"\\U0001F680-\\U0001F6FF\": \"Transport & Map Symbols\",\n",
    "       \n",
    "        u\"\\U0001F4FF\": \"Praying Hands (Not official emoji)\"\n",
    "    }\n",
    "\n",
    "    return emozi_names\n",
    "\n",
    "emozi_names = get_emozi_names()\n",
    "\n",
    "print(\"\\nEmoji names:\")\n",
    "for pattern, name in emozi_names.items():\n",
    "    print(f\"Pattern: {pattern}, Name: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fa2be1c",
   "metadata": {},
   "source": [
    "# TOKENIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbb1e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING THE SPLIT FUNCTION\n",
    "SENT1='I am going to patna'\n",
    "SENT1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bac26ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#USING THE SPLIT FUNCTION\n",
    "SENT1='I am going to patna!'\n",
    "SENT1.split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4bd7388",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "SENTENCE = \"I AM GOING TO PATNA!\"\n",
    "tokens = re.findall(r'\\b\\w+\\b', SENTENCE)\n",
    "print(tokens)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1500dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NLTK\n",
    "from nltk.tokenize import word_tokenize,sent_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ce9afa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent1=\"i am going to patna!\"\n",
    "print(word_tokenize(sent1))\n",
    "print(sent_tokenize(sent1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52637c2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "email_sentence = \"Contact me at example@gmail.com.\"\n",
    "education_sentence = \"A Ph.D in A.I. requires extensive research.\"\n",
    "news_sentence = \"The stock market experienced a sharp decline today.\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559fa385",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(email_sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbcda90",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(education_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c45a312",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_tokenize(news_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb162611",
   "metadata": {},
   "outputs": [],
   "source": [
    "##USING SPACY LIBERARY\n",
    "import spacy\n",
    "\n",
    "# Load the English core web model\n",
    "nlp = spacy.load('en_core_web_sm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e879956",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc1=nlp(email_sentence)\n",
    "doc2=nlp(education_sentence)\n",
    "doc3=nlp(news_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514265dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc1:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6bb732",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for token in doc2:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addce993",
   "metadata": {},
   "outputs": [],
   "source": [
    "for token in doc3:\n",
    "    print(token)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265d51c7",
   "metadata": {},
   "source": [
    "# STEMMING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb997f73",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"@@@ ABOUT STEMMING @@@\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8e6c9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "print('''IN GRAMMAR,INFLECTION IS THE MODIFICATION OF A WORD TO EXPRESS DIFFERENT GRAMMATICAL CATEGORIES SUCH AS TENSE,CASE,VOICE,ASPECT,PERSON,NUMBER,GENDER,AND MOOD.\\n\n",
    "PREFIX->CHARACTER(S)AT THE BEGINNING.\\n\n",
    "SUFFIX->CHARACTER(S)AT THE END.\\n\n",
    "INFIX->CHARACTER(S)AT IN BETWEEN.\\n\n",
    "EXCEPTION->SPECIAL-CASE RULE TO SPLIT A STRING INTO SEVERAL TOKENS OR PREVENT A TOKEN FROM BEING SPLIT WHEN PUNCTUATION RULES ARE APPLIED.\\n''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cce50b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('''Stemming is the process of reducing inflection in words to their root forms such as mapping a group of words to \n",
    "the same stem even if the stem itself is not a valid word in the language ''')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be1dccb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "ps=PorterStemmer()\n",
    "def stem_words(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "print('@@@@ WITHOUT STEM APPLY  @@@@\\n')\n",
    "print(sample,'\\n')\n",
    "sample=\"run running runs walked \"\n",
    "print('@@@@ AFTER STEM APPLY @@@@\\n')\n",
    "stem_words(sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f88ab8",
   "metadata": {},
   "source": [
    "# LEMMATIZATION\n",
    "''' Lemmatization , unlike stemming ,reduces the inflected words properly ensuring that the root word belongs to the language. In lemmatization  root word is called lemma .a lemma (plural lemmas or lemmata)is the canonical from dictionary form ,or citation form of a set of words. '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f631924",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "wordnet_lemmatizer=WordNetLemmatizer()\n",
    "sentence=\" she is beutiful girl and her smile is very pretty and she is eating food!.\"\n",
    "punctuation=\":?!.,\"\n",
    "sentence_words=nltk.word_tokenize(sentence)\n",
    "for word in sentence_words:\n",
    "    if word in sentence_words:\n",
    "        sentence_words.remove(word)\n",
    "        \n",
    "sentence_words\n",
    "print(\"{0:20}{1:20}\" .format('word','Lemma'))\n",
    "for word in sentence_words:\n",
    "    print(\"{0:20}{1:20}\".format(word,wordnet_lemmatizer.lemmatize(word,pos='v')))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
